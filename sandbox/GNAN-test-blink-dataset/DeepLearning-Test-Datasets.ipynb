{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuutyCx4YTpX"
   },
   "source": [
    "# Data Mining Lab 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "anfjcPSSYTpX"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the .mat files\n",
    "mat_invol = scipy.io.loadmat('epochs_blinks_ASR_invol.mat')\n",
    "mat_vol = scipy.io.loadmat('epochs_blinks_ASR_vol.mat')\n",
    "\n",
    "# Extract the data from the .mat files\n",
    "data_invol = mat_invol['epochs_blinks_ASR_invol']\n",
    "data_vol = mat_vol['epochs_blinks_ASR_vol']\n",
    "\n",
    "# Number of features ('FP1' and 'FP2'), time steps and samples\n",
    "n_features = data_invol.shape[0]\n",
    "n_timesteps = data_invol.shape[1]\n",
    "n_samples_invol = data_invol.shape[2]\n",
    "n_samples_vol = data_vol.shape[2]\n",
    "\n",
    "# Reshape the data to be of size (samples, timesteps, features)\n",
    "data_invol = np.transpose(data_invol, (2, 1, 0))\n",
    "data_vol = np.transpose(data_vol, (2, 1, 0))\n",
    "\n",
    "# Create labels for the involuntary (0) and voluntary (1) data\n",
    "labels_invol = np.zeros((n_samples_invol, 1))\n",
    "labels_vol = np.ones((n_samples_vol, 1))\n",
    "\n",
    "# Combine the involuntary and voluntary data and labels\n",
    "X = np.concatenate((data_invol, data_vol), axis=0)\n",
    "y = np.concatenate((labels_invol, labels_vol), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yVc2T5MIYTpX"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, Bidirectional\n",
    "from keras.layers import MaxPooling1D, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and temporary sets (70% training, 30% temporary)\n",
    "X_temp, X_train, y_temp, y_train = train_test_split(X, y, test_size=0.7, random_state=42, shuffle=True)\n",
    "\n",
    "# Split the temporary set into validation and test sets (50% validation, 50% test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Kw8bGMv7YTpX",
    "outputId": "9f6f7052-302e-4794-ef69-b84450b61b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.6338 - accuracy: 0.6378 - val_loss: 0.8603 - val_accuracy: 0.5662\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.5380 - accuracy: 0.7291 - val_loss: 0.7397 - val_accuracy: 0.6029\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.5061 - accuracy: 0.7780 - val_loss: 0.6577 - val_accuracy: 0.5956\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.4757 - accuracy: 0.7811 - val_loss: 0.7083 - val_accuracy: 0.6029\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.4586 - accuracy: 0.7969 - val_loss: 0.6502 - val_accuracy: 0.6324\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.4314 - accuracy: 0.8063 - val_loss: 0.5925 - val_accuracy: 0.6912\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.4076 - accuracy: 0.8205 - val_loss: 0.5704 - val_accuracy: 0.6765\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.4012 - accuracy: 0.8252 - val_loss: 0.5697 - val_accuracy: 0.6765\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 30s 2s/step - loss: 0.3549 - accuracy: 0.8504 - val_loss: 0.6383 - val_accuracy: 0.6618\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.3446 - accuracy: 0.8646 - val_loss: 0.6586 - val_accuracy: 0.6471\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(32, 5, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "model.add(MaxPooling1D(1))\n",
    "\n",
    "# Batch normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# LSTM layer\n",
    "model.add(Bidirectional(LSTM(25, return_sequences=True)))  # Bidirectional LSTM layer with 50 units\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(Bidirectional(LSTM(50)))  # Another Bidirectional LSTM layer with 50 units\n",
    "\n",
    "# Output layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "learning_rate = 0.002  # Set the learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data and validate on the validation data\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Test the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HBHwcL8sYTpX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 600ms/step - loss: 0.7283 - accuracy: 0.6618\n",
      "Test loss: 0.7282834649085999, Test accuracy: 0.6617646813392639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "\n",
    "# predict the values from the test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if y >= 0.5 else 0 for y in y_pred]  # transform probabilities into binary outputs\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted Voluntary Blinking')\n",
    "plt.ylabel('True Voluntary Blinking')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9w_cDUwCYTpX",
    "outputId": "3582ac44-1f5f-4cb2-b833-d477f152461a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (3613, 4)\n",
      "Shape of Testing df:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "def calculate_psd(data, Fs):\n",
    "    \"\"\"\n",
    "    Calculate the power spectral density (PSD) for EEG data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array-like, EEG data with shape (channels, points, trials)\n",
    "    - Fs: int, the sampling frequency\n",
    "\n",
    "    Returns:\n",
    "    - psds: array-like, PSDs with shape (channels, frequencies, trials)\n",
    "    - freqs: array-like, frequencies for which the PSDs were computed\n",
    "    \"\"\"\n",
    "    # Initialize array to store the PSDs\n",
    "    psds = []\n",
    "\n",
    "    # Loop over the channels\n",
    "    for channel_data in data:\n",
    "        # Loop over the trials\n",
    "        channel_psds = []\n",
    "        for trial_data in channel_data.T:\n",
    "            # Calculate the PSD using Welch's method\n",
    "            freqs, psd = signal.welch(trial_data, Fs)\n",
    "            channel_psds.append(psd)\n",
    "        psds.append(channel_psds)\n",
    "    \n",
    "    return np.array(psds), freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the .mat files\n",
    "data_invol = mat_invol['epochs_blinks_ASR_invol']\n",
    "data_vol = mat_vol['epochs_blinks_ASR_vol']\n",
    "​\n",
    "Fs=250; #sample frequency\n",
    "# Calculate the PSDs of the GNAN data\n",
    "psds_invol, freqs = calculate_psd(data_invol, Fs)\n",
    "psds_vol, _ = calculate_psd(data_vol, Fs)\n",
    "​\n",
    "# Number of features ('FP1' and 'FP2'), time steps and samples\n",
    "n_features = psds_invol.shape[0]\n",
    "n_timesteps = psds_invol.shape[2]\n",
    "n_samples_invol = psds_invol.shape[1]\n",
    "n_samples_vol = psds_vol.shape[1]\n",
    "​\n",
    "# Reshape the data to be of size (samples, timesteps, features)\n",
    "psds_invol = np.transpose(psds_invol, (1, 2, 0))\n",
    "psds_vol = np.transpose(psds_vol, (1, 2, 0))\n",
    "​\n",
    "# Create labels for the involuntary (0) and voluntary (1) data\n",
    "labels_invol = np.zeros((n_samples_invol, 1))\n",
    "labels_vol = np.ones((n_samples_vol, 1))\n",
    "​\n",
    "# Combine the involuntary and voluntary data and labels\n",
    "X = np.concatenate((psds_invol, psds_vol), axis=0)\n",
    "y = np.concatenate((labels_invol, labels_vol), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (70% training, 30% temporary)\n",
    "X_temp, X_train, y_temp, y_train = train_test_split(X, y, test_size=0.7, random_state=42, shuffle=True)\n",
    "\n",
    "# Split the temporary set into validation and test sets (50% validation, 50% test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(128, 3, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "# Batch normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# LSTM layer\n",
    "model.add(Bidirectional(LSTM(25, return_sequences=True)))  # Bidirectional LSTM layer with 50 units\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(Bidirectional(LSTM(75)))  # Another Bidirectional LSTM layer with 50 units\n",
    "\n",
    "# Output layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "learning_rate = 0.003  # Set the learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data and validate on the validation data\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val))\n",
    "\n",
    "# Test the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the values from the test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if y >= 0.5 else 0 for y in y_pred]  # transform probabilities into binary outputs\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted Voluntary Blinking')\n",
    "plt.ylabel('True Voluntary Blinking')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .mat files\n",
    "mat_invol_jap = scipy.io.loadmat('/kaggle/input/jap-epoch-raw-data/jap_blink_involuntary.mat')\n",
    "mat_vol_jap = scipy.io.loadmat('/kaggle/input/jap-epoch-raw-data/jap_blink_voluntary.mat')\n",
    "\n",
    "# Extract the data from the .mat files\n",
    "data_invol_jap = mat_invol_jap['jap_blink_involuntary']\n",
    "data_vol_jap = mat_vol_jap['jap_blink_voluntary']\n",
    "\n",
    "# Number of features ('FP1' and 'FP2'), time steps and samples\n",
    "n_features_jap = data_invol_jap.shape[0]\n",
    "n_timesteps_jap = data_invol_jap.shape[1]\n",
    "n_samples_invol_jap = data_invol_jap.shape[2]\n",
    "n_samples_vol_jap = data_vol_jap.shape[2]\n",
    "\n",
    "# Reshape the data to be of size (samples, timesteps, features)\n",
    "data_invol_jap = np.transpose(data_invol_jap, (2, 1, 0))\n",
    "data_vol_jap = np.transpose(data_vol_jap, (2, 1, 0))\n",
    "\n",
    "# Create labels for the involuntary (0) and voluntary (1) data\n",
    "labels_invol_jap = np.zeros((n_samples_invol_jap, 1))\n",
    "labels_vol_jap = np.ones((n_samples_vol_jap, 1))\n",
    "\n",
    "# Combine the involuntary and voluntary data and labels\n",
    "X_jap = np.concatenate((data_invol_jap, data_vol_jap), axis=0)\n",
    "y_jap = np.concatenate((labels_invol_jap, labels_vol_jap), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (70% training, 30% temporary)\n",
    "X_temp_jap, X_train_jap, y_temp_jap, y_train_jap = train_test_split(X_jap, y_jap, test_size=0.7, random_state=42, shuffle=True)\n",
    "\n",
    "# Split the temporary set into validation and test sets (50% validation, 50% test)\n",
    "X_val_jap, X_test_jap, y_val_jap, y_test_jap = train_test_split(X_temp_jap, y_temp_jap, test_size=0.5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_jap = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model_jap.add(Conv1D(64, 3, activation='relu', input_shape=(n_timesteps_jap, n_features_jap)))\n",
    "model_jap.add(MaxPooling1D(3))\n",
    "\n",
    "# Batch normalization layer\n",
    "model_jap.add(BatchNormalization())\n",
    "\n",
    "# LSTM layer\n",
    "model_jap.add(Bidirectional(LSTM(25, return_sequences=True)))  # Bidirectional LSTM layer with 50 units\n",
    "\n",
    "# Dropout for regularization\n",
    "model_jap.add(Dropout(0.5))\n",
    "\n",
    "# LSTM layer\n",
    "model_jap.add(Bidirectional(LSTM(50)))  # Another Bidirectional LSTM layer with 50 units\n",
    "\n",
    "# Output layer for binary classification\n",
    "model_jap.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "learning_rate = 0.00021  # Set the learning rate\n",
    "optimizer_jap = Adam(learning_rate=learning_rate)\n",
    "model_jap.compile(loss='binary_crossentropy', optimizer=optimizer_jap, metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data and validate on the validation data\n",
    "history_jap = model_jap.fit(X_train_jap, y_train_jap, epochs=100, batch_size=32, validation_data=(X_val_jap, y_val_jap))\n",
    "\n",
    "# Test the model on the test data\n",
    "test_loss_jap, test_accuracy_jap = model_jap.evaluate(X_test_jap, y_test_jap)\n",
    "print(f'Test loss: {test_loss_jap}, Test accuracy: {test_accuracy_jap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the values from the test dataset\n",
    "y_pred_jap = model_jap.predict(X_test_jap)\n",
    "y_pred_jap = [1 if y_jap >= 0.5 else 0 for y_jap in y_pred_jap]  # transform probabilities into binary outputs\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_mat_jap = confusion_matrix(y_test_jap, y_pred_jap)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_mat_jap, annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted Voluntary Blinking')\n",
    "plt.ylabel('True Voluntary Blinking')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report_jap = classification_report(y_test_jap, y_pred_jap)\n",
    "\n",
    "print(report_jap)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4e5eiVLOYTp5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 594.85,
   "position": {
    "height": "40px",
    "left": "723px",
    "right": "20px",
    "top": "80px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
